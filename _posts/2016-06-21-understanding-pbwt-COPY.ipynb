{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some data. The variable *X* below is a list of 8 haplotypes with 6 variable sites, all biallelic. This is just some dummy data I made up myself to test out the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = [[0, 1, 0, 1, 0, 0],\n",
    "     [1, 1, 0, 0, 1, 1],\n",
    "     [1, 0, 1, 1, 1, 0],\n",
    "     [0, 1, 1, 1, 0, 0],\n",
    "     [1, 1, 1, 0, 1, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Algorithm 1. Build prefix array\n",
    "\n",
    "The basic idea of PBWT is to sort the haplotypes in order of reversed prefixes. This sorting then enables matches between haplotypes to be found efficiently. Algorithm 1 in Durbin (2014) builds something called the \"positional prefix array\", which is just the list of haplotype indices that would sort the haplotypes in reversed prefix order at some position *k*. \n",
    "\n",
    "Durbin uses the notation *a<sub>k</sub>* to denote the positional prefix array for position *k*, however he also uses the variable *a* for one of the intermediates within algorithm 1, so to avoid any ambiguity here I will use *ppa<sub>k</sub>* to denote the positional prefix array at position *k*.\n",
    "\n",
    "The trick of algorithm 1 is to sweep through the data by position, building *ppa<sub>k+1</sub>* from *ppa<sub>k</sub>*. The algorithm simply sorts the haplotypes by allele at position *k*, making sure that the sort order from the previous position is retained for all haplotypes with the same allele value.\n",
    "\n",
    "Here is a pure Python implementation of algorithm 1, purely for illustration purposes.\n",
    "\n",
    "**Benjamin** : extended to return the complete list of *a<sub>k</sub>* (*ppa<sub>k</sub>* for all *k*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_all_prefix_array(X):\n",
    "    \n",
    "    # M haplotypes\n",
    "    M = len(X)\n",
    "\n",
    "    # N variable sites\n",
    "    N = len(X[0])\n",
    "    \n",
    "    all_ppa = list()\n",
    "    \n",
    "    # initialise positional prefix array\n",
    "    ppa = list(range(M))\n",
    "    \n",
    "    # iterate over variants\n",
    "    for k in range(N-1):\n",
    "\n",
    "        # setup intermediates\n",
    "        a = list()\n",
    "        b = list()\n",
    "    \n",
    "        # iterate over haplotypes in reverse prefix sorted order\n",
    "        for index in ppa:\n",
    "\n",
    "            # current haplotype\n",
    "            haplotype = X[index]\n",
    "            \n",
    "            # allele for current haplotype\n",
    "            allele = haplotype[k]\n",
    "            \n",
    "            # update intermediates\n",
    "            if allele == 0:\n",
    "                a.append(index)\n",
    "            else:\n",
    "                b.append(index)\n",
    "                \n",
    "        # construct the new positional prefix array for k+1 by concatenating lists a and b\n",
    "        ppa = a + b\n",
    "        all_ppa.append(ppa)\n",
    "        \n",
    "    return all_ppa\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it on the dummy data *X*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "res = build_all_prefix_array(X)\n",
    "\n",
    "# list of a_k\n",
    "print(\"\\n\".join([\"a\"+str(k)+\": \"+str(ele) for k,ele in enumerate(res)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting list of integers is the list of haplotype indices that sorts the haplotypes up to the final position (*k* = *N*-1). To help visualise this, let's write a display function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prefix_array(X):\n",
    "    from IPython.display import display_html\n",
    "    ppa = build_all_prefix_array(X)[len(X[0])-2]\n",
    "    html = '<pre style=\"line-height: 100%\">'\n",
    "    for index in ppa:\n",
    "        haplotype = X[index]\n",
    "        html += str(index) + '|' + ''.join(str(allele) for allele in haplotype[:-1])\n",
    "        html += '  ' + str(haplotype[-1]) + '<br/>'\n",
    "    html += '</pre>'\n",
    "    display_html(html, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_prefix_array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positional prefix array is the column to the left of the '``|``' showing the haplotype indices. To the right of the '``|``' are the haplotypes up to *k* = *N*-1 sorted in reversed prefix order. Then separated by a space is the next allele for each haplotype - this final column makes up what Durbin (2014) calls *y<sup>k</sup>*[*k*]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2. Build prefix and divergence arrays\n",
    "\n",
    "Sorting haplotypes in this order has a very useful property, which is that each haplotype will be adjacent to the haplotype with the longest match. If several haplotypes have equally long matches these will all be adjacent. Also, the length of match between any pair of non-adjacent haplotypes is the minimum of the lengths of the matches between all haplotypes occurring in between them in the sorted order.\n",
    "\n",
    "Algorithm 2 shows how to find and keep track of the start position for matches between adjacent haplotypes. This can be done while sweeping through the data building the positional prefix arrays. The reason this is possible is because once a match begins between a pair of adjacent haplotypes, those two haplotypes will remain adjacent until the match breaks.\n",
    "\n",
    "Here's a pure Python implementation of algorithm 2, again purely for illustration. Durbin (2014) uses *d<sub>k</sub>* to denote the \"divergence array\" at position *k*, where *d<sub>k</sub>*[*i*] is the position where a match begins between the *i*th haplotype in the sorted order and it's predecessor. Because *d* is also used for one of the intermediates, for clarity I'll use *div<sub>k</sub>* to denote the divergence array.\n",
    "\n",
    "**Benjamin** : extended to return the complete list of *a<sub>k</sub>* and *d<sub>k</sub>* for all *k*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_prefix_and_divergence_arrays(X):\n",
    "    \n",
    "    # M haplotypes\n",
    "    M = len(X)\n",
    "\n",
    "    # N variable sites\n",
    "    N = len(X[0])\n",
    "    \n",
    "    # initialise positional prefix array\n",
    "    ppa = list(range(M))\n",
    "    \n",
    "    # initialise divergence array\n",
    "    div = [0] * M\n",
    "\n",
    "    #keep track for all k\n",
    "    all_ppa = list()\n",
    "    all_div = list()\n",
    "\n",
    "    \n",
    "    # iterate over variants\n",
    "    for k in range(N-1):\n",
    "\n",
    "        # setup intermediates\n",
    "        a = list()\n",
    "        b = list()\n",
    "        d = list()\n",
    "        e = list()\n",
    "        p = q = k + 1\n",
    "    \n",
    "        # iterate over haplotypes in reverse prefix sorted order\n",
    "        for index, match_start in zip(ppa, div):\n",
    "\n",
    "            # current haplotype\n",
    "            haplotype = X[index]\n",
    "            \n",
    "            # allele for current haplotype\n",
    "            allele = haplotype[k]\n",
    "            \n",
    "            # update intermediates\n",
    "            if match_start > p:\n",
    "                p = match_start\n",
    "            if match_start > q:\n",
    "                q = match_start\n",
    "\n",
    "            # update intermediates\n",
    "            if allele == 0:\n",
    "                a.append(index)\n",
    "                d.append(p)\n",
    "                p = 0\n",
    "            else:\n",
    "                b.append(index)\n",
    "                e.append(q)\n",
    "                q = 0\n",
    "                \n",
    "        # construct the new arrays for k+1 by concatenating intermediates\n",
    "        ppa = a + b\n",
    "        div = d + e\n",
    "        all_ppa.append(ppa)\n",
    "        all_div.append(div)\n",
    "    \n",
    "    return all_ppa, all_div\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out on the dummy haplotype data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ppa, all_div = build_all_prefix_and_divergence_arrays(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join([\"a[\"+str(k)+\"]: \"+str(ele) for k,ele in enumerate(all_ppa)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join([\"d_k[\"+str(k)+\"]: \"+str(ele) for k,ele in enumerate(all_div)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again to help make sense of this let's write a display function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prefix_and_divergence_arrays(X):\n",
    "    from IPython.display import display_html\n",
    "    all_ppa, all_div = build_all_prefix_and_divergence_arrays(X)\n",
    "    ppa = all_ppa[len(X[0])-2]\n",
    "    div = all_div[len(X[0])-2]\n",
    "    html = '<pre style=\"line-height: 100%\">'\n",
    "    html += 'i|d[N-1]|' + \"\".join([\" \" for i in range(len(X[0]))]) + 'k=N-1<br/>'\n",
    "    for index, match_start in zip(ppa, div):\n",
    "        haplotype = X[index]\n",
    "        html += str(index) + '|' + \"\".join(\" \" for i in range(6-len(str(match_start))))\n",
    "        html += str(match_start) + '|'        \n",
    "        for k, allele in enumerate(haplotype[:-1]):\n",
    "            if match_start == k:\n",
    "                html += '<strong><u>'\n",
    "            html += str(allele)\n",
    "        if match_start < len(haplotype) - 1:\n",
    "            html += '</u></strong>'\n",
    "        html += '  ' + str(haplotype[-1]) + '<br/>'\n",
    "    html += '</pre>'\n",
    "    display_html(html, raw=True)\n",
    "    \n",
    "display_prefix_and_divergence_arrays(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The divergence array has been used to show underlined in bold the maximal matches between each haplotype and it's predecessor in the sorted order, as in Durbin (2014) Figure 1. \n",
    "\n",
    "Let's try it out on some randomly generated data, just to have another example to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "M2 = 10\n",
    "N2 = 15\n",
    "Y = [[random.randint(0, 1) for _ in range(N2)] for _ in range(M2)]\n",
    "all_ppa, all_div = build_all_prefix_and_divergence_arrays(Y)\n",
    "display_prefix_and_divergence_arrays(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 3. Report long matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 3 shows how to find all matches between haplotypes longer than some value *L*. The trick here is to notice that all haplotypes with matches longer than L will be adjacent in the sorted order, forming a \"block\". So algorithm 3 iterates over the haplotypes in sorted order, collecting haplotypes with matches longer than L. When it finds a match less than *L*, it must mean a break between blocks, so it reports matches for all haplotypes in the previous block. The algorithm also collects separately for haplotypes with different alleles at position *k*, to ensure that only matches that terminate at *k* are reported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_long_matches(X, L):\n",
    "    \n",
    "    # M haplotypes\n",
    "    M = len(X)\n",
    "\n",
    "    # N variable sites\n",
    "    N = len(X[0])\n",
    "    \n",
    "    # initialise positional prefix array\n",
    "    ppa = list(range(M))\n",
    "    \n",
    "    # initialise divergence array\n",
    "    div = [0] * M\n",
    "    \n",
    "    # iterate over variants\n",
    "    for k in range(N):\n",
    "\n",
    "        # setup intermediates\n",
    "        a = list()\n",
    "        b = list()\n",
    "        d = list()\n",
    "        e = list()\n",
    "        p = q = k + 1\n",
    "        ma = list()\n",
    "        mb = list()\n",
    "    \n",
    "        # iterate over haplotypes in reverse prefix sorted order\n",
    "        for index, match_start in zip(ppa, div):\n",
    "            \n",
    "            # report matches\n",
    "            if match_start > k - L:\n",
    "                if ma and mb:\n",
    "                    yield k, ma, mb\n",
    "                ma = list()\n",
    "                mb = list()\n",
    "\n",
    "            # current haplotype\n",
    "            haplotype = X[index]\n",
    "            \n",
    "            # allele for current haplotype\n",
    "            allele = haplotype[k]\n",
    "            \n",
    "            # update intermediates\n",
    "            if match_start > p:\n",
    "                p = match_start\n",
    "            if match_start > q:\n",
    "                q = match_start\n",
    "\n",
    "            # update intermediates\n",
    "            if allele == 0:\n",
    "                a.append(index)\n",
    "                d.append(p)\n",
    "                p = 0\n",
    "                ma.append(index)\n",
    "            else:\n",
    "                b.append(index)\n",
    "                e.append(q)\n",
    "                q = 0\n",
    "                mb.append(index)\n",
    "                \n",
    "        # report any remaining matches including final haplotype (N.B., not in Durbin 2014)\n",
    "        if ma and mb:\n",
    "            yield k, ma, mb\n",
    "                \n",
    "        # construct the new arrays for k+1\n",
    "        if k < N - 1:\n",
    "            ppa = a + b\n",
    "            div = d + e\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One minor note, I don't think algorithm 3 as presented in Durbin (2014) accounts for the case where a block of matching haplotypes extends up to the final haplotype, so I added in an extra couple of lines to account for this case.\n",
    "\n",
    "Let's try it out on the dummy data, finding matches 3 or longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_prefix_and_divergence_arrays(X)\n",
    "for match in report_long_matches(X, 2):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 4 matches have been found. The first match terminates at position 4, and is between haplotypes 4 and 5. Again let's write a display function to help visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def display_long_matches(X, L):\n",
    "    from IPython.display import display_html\n",
    "    html = '<pre style=\"line-height: 100%\">'\n",
    "    for match in report_long_matches(X, L):\n",
    "        k, ma, mb = match\n",
    "        for i in sorted(ma):\n",
    "            for j in sorted(mb):\n",
    "                html += 'match ending at position %s between haplotypes %s and %s:<br/><br/>' % (k, i, j)\n",
    "                h1 = X[i][k-L:k+1]\n",
    "                h2 = X[j][k-L:k+1]\n",
    "                html += str(i) + '|' + ''.join(str(allele) for allele in h1[:-1]) \n",
    "                html += str(h1[-1]) + '<br/>'\n",
    "                html += str(j) + '|<strong><u>' + ''.join(str(allele) for allele in h2[:-1]) + '</u></strong>'\n",
    "                html += str(h2[-1]) + '<br/><br/>'\n",
    "    html += '</pre>'\n",
    "    display_html(html, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_long_matches(X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_long_matches(X, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_long_matches(X, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the dummy data *X* contains two haplotypes that are completely identical over all 6 positions, so why aren't these found? That's because the algorithm requires matches to terminate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 4. Report set maximal matches\n",
    "\n",
    "Algorithm 4 shows how to find \"set maximal matches\" for each haplotype. These are the longest match for each haplotype at each position *k*. Again this makes use of the fact that haplotypes with maximal matches will be adjacent in the sorted order. At each position *k*, the algorithm iterates through the haplotypes in sorted order, looking for a maximal match for each haplotype. The first thing it does is to try and find any neighbours where the match can be extended, i.e., where the alleles at position *k* are the same. If so, continue on to the next haplotype without reporting anything. Otherwise, report a match with the longest matching neighbour (taking into account the fact that several neighbours may have equally long matches). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_set_maximal_matches(X):\n",
    "\n",
    "    # M haplotypes\n",
    "    M = len(X)\n",
    "\n",
    "    # N variable sites\n",
    "    N = len(X[0])\n",
    "    \n",
    "    # initialise positional prefix array\n",
    "    ppa = list(range(M))\n",
    "    \n",
    "    # initialise divergence array\n",
    "    div = [0] * M\n",
    "    \n",
    "    # iterate over variants\n",
    "    for k in range(N):\n",
    "        \n",
    "        # sentinel\n",
    "        div.append(k+1)\n",
    "        \n",
    "        for i in range(M):\n",
    "            \n",
    "            m = i - 1\n",
    "            n = i + 1\n",
    "            match_continues = False\n",
    "            \n",
    "            if div[i] <= div[i+1]:\n",
    "                # match to previous neighbour is longer, scan \"down\" the haplotypes (decreasing indices)\n",
    "                while div[m+1] <= div[i]:\n",
    "                    if X[ppa[m]][k] == X[ppa[i]][k]:\n",
    "                        match_continues = True\n",
    "                        break\n",
    "                    m -= 1\n",
    "            if match_continues:\n",
    "                continue\n",
    "                    \n",
    "            if div[i] >= div[i+1]:\n",
    "                # match to next neighbour is longer, scan \"up\" the haplotypes (increasing indices)\n",
    "                while div[n] <= div[i+1]:\n",
    "                    if X[ppa[n]][k] == X[ppa[i]][k]:\n",
    "                        match_continues = True\n",
    "                        break\n",
    "                    n += 1\n",
    "            if match_continues:\n",
    "                continue\n",
    "                \n",
    "            for j in range(m+1, i):\n",
    "                if div[i] < k:  # N.B., avoid 0 length matches, not in Durbin (2014)\n",
    "                    yield ppa[i], ppa[j], div[i], k\n",
    "                \n",
    "            for j in range(i+1, n):\n",
    "                if div[i+1] < k:  # N.B., avoid 0 length matches, not in Durbin (2014)\n",
    "                    yield ppa[i], ppa[j], div[i+1], k\n",
    "                    \n",
    "        # build next prefix and divergence arrays\n",
    "        if k < N - 1:        \n",
    "                \n",
    "            # setup intermediates\n",
    "            a = list()\n",
    "            b = list()\n",
    "            d = list()\n",
    "            e = list()\n",
    "            p = q = k + 1\n",
    "\n",
    "            # iterate over haplotypes in prefix sorted order\n",
    "            for index, match_start in zip(ppa, div):\n",
    "\n",
    "                # current haplotype\n",
    "                haplotype = X[index]\n",
    "\n",
    "                # allele for current haplotype\n",
    "                allele = haplotype[k]\n",
    "\n",
    "                # update intermediates\n",
    "                if match_start > p:\n",
    "                    p = match_start\n",
    "                if match_start > q:\n",
    "                    q = match_start\n",
    "\n",
    "                # update intermediates\n",
    "                if allele == 0:\n",
    "                    a.append(index)\n",
    "                    d.append(p)\n",
    "                    p = 0\n",
    "                else:\n",
    "                    b.append(index)\n",
    "                    e.append(q)\n",
    "                    q = 0\n",
    "\n",
    "            # construct the new arrays for k+1\n",
    "            ppa = a + b\n",
    "            div = d + e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for match in report_set_maximal_matches(X):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_set_maximal_matches(X):\n",
    "    from IPython.display import display_html\n",
    "    from operator import itemgetter\n",
    "    from itertools import groupby\n",
    "    html = '<pre style=\"line-height: 100%\">'\n",
    "    matches = sorted(report_set_maximal_matches(X), key=itemgetter(0, 2))\n",
    "    for i, sub_matches in groupby(matches, key=itemgetter(0)):\n",
    "        html += 'set maximal matches for haplotype %s:<br/><br/>' % i\n",
    "        hi = X[i]\n",
    "        html += str(i) + '|' + ''.join(map(str, hi)) + '<br/>'\n",
    "        for _, j, k1, k2 in sub_matches:\n",
    "            hj = X[j]\n",
    "            html += str(j) + '|' + ''.join(map(str, hj[:k1]))\n",
    "            html += '<strong><u>' + ''.join(map(str, hj[k1:k2])) + '</u></strong>'\n",
    "            html += ''.join(map(str, hj[k2:])) + '<br/>'\n",
    "        html += '<br/>'\n",
    "    html += '</pre>'\n",
    "    display_html(html, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_set_maximal_matches(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the bold underline indicates the regions in the haplotype representing set maximal matches to the haplotype at the top.\n",
    "\n",
    "Again you may notice that there are no matches reported for the two haplotypes that are identical across all 6 positions. Again this is because matches are required to terminate.\n",
    "\n",
    "Let's have a look with some random data for further illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "M = 3\n",
    "N = 30\n",
    "Y = [[random.randint(0, 1) for _ in range(N)] for _ in range(M)]\n",
    "display_set_maximal_matches(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 5. Set maximal matches from a new sequence *z* to *X*\n",
    "\n",
    "I haven't got my head around this yet. Any help with the basic intuition would be very welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compact representation of *X*\n",
    "\n",
    "I think I partly get this, at least why *PBWT* should be more compressible than the original *X*. But I don't get how you recover *X* from *PBWT*. I need to go and read up about the FM-index.\n",
    "\n",
    "Let's at least check the assertion that the *PBWT* is more compressible than *X*, using some real haplotype data from [Ag1000G](http://www.malariagen.net/ag1000g). First, I'm going to implement algorithm 2 in a slightly different way, using NumPy and Cython to speed things up, and also constructing the *PBWT* as output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "\n",
    "def build_pbwt(np.int8_t[:, :] H):\n",
    "    cdef:\n",
    "        Py_ssize_t N, M, k, i, u, v, p, q\n",
    "        np.uint32_t index, match_start\n",
    "        np.int8_t allele\n",
    "        np.int8_t[:, :] pbwt\n",
    "        np.uint32_t[:, :] ppa, div\n",
    "        np.uint32_t[:] a, b, d, e\n",
    "    \n",
    "    # expect haplotype data transposed\n",
    "    N, M = H.shape[:2]\n",
    "    \n",
    "    # setup pbwt\n",
    "    pbwt = np.empty_like(H)\n",
    "\n",
    "    # setup positional prefix array\n",
    "    ppa = np.empty((N, M), dtype='u4')\n",
    "    \n",
    "    # initialise first ppa column\n",
    "    for i in range(M):\n",
    "        ppa[0, i] = i\n",
    "    \n",
    "    # setup divergence array\n",
    "    div = np.zeros((N, M), dtype='u4')\n",
    "    \n",
    "    # setup intermediates\n",
    "    a = np.zeros(M, dtype='u4')\n",
    "    b = np.zeros(M, dtype='u4')\n",
    "    d = np.zeros(M, dtype='u4')\n",
    "    e = np.zeros(M, dtype='u4')\n",
    "    \n",
    "    # iterate over variants\n",
    "    for k in range(N):\n",
    "        \n",
    "        # setup intermediates\n",
    "        u = v = 0\n",
    "        p = q = k + 1\n",
    "    \n",
    "        # iterate over haplotypes in reverse prefix sorted order\n",
    "        for i in range(M):\n",
    "\n",
    "            # index for current haplotype\n",
    "            index = ppa[k, i]\n",
    "            \n",
    "            # match start position for current haplotype\n",
    "            match_start = div[k, i]\n",
    "            \n",
    "            # allele for current haplotype\n",
    "            allele = H[k, index]\n",
    "            \n",
    "            # update output\n",
    "            pbwt[k, i] = allele\n",
    "            \n",
    "            # update intermediates\n",
    "            if match_start > p:\n",
    "                p = match_start\n",
    "            if match_start > q:\n",
    "                q = match_start\n",
    "\n",
    "            # update intermediates\n",
    "            if allele == 0:\n",
    "                a[u] = index\n",
    "                d[u] = p\n",
    "                u += 1\n",
    "                p = 0\n",
    "            else:\n",
    "                b[v] = index\n",
    "                e[v] = q\n",
    "                v += 1\n",
    "                q = 0\n",
    "                \n",
    "        if k < N - 1:\n",
    "            \n",
    "            # construct the new positional prefix array for k+1\n",
    "            ppa[k+1, :u] = a[:u]\n",
    "            ppa[k+1, u:] = b[:v]\n",
    "        \n",
    "            # construct the new divergence array for k+1\n",
    "            div[k+1, :u] = d[:u]\n",
    "            div[k+1, u:] = e[:v]\n",
    "        \n",
    "    return np.asarray(pbwt), np.asarray(ppa), np.asarray(div)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some extra libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import allel; print('scikit-allel', allel.__version__)\n",
    "import zarr; print('zarr', zarr.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some real haplotype data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "callset_fn = 'data/2016-06-21/ag1000g.phase1.ar3.haplotypes.3R.h5'\n",
    "callset = h5py.File(callset_fn, mode='r')\n",
    "callset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "genotypes = allel.GenotypeChunkedArray(callset['3R/calldata/genotype'])\n",
    "genotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison with results from Durbin (2014) let's take 1000 haplotypes and use the same number of segregating sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "H = genotypes[:500000, :500].to_haplotypes()\n",
    "ac = H.count_alleles()\n",
    "H = H[ac.is_segregating()]\n",
    "H = H[:370264]\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the PBWT transform, check how compressible these data are already, using a standard compression algorithm (Zstsandard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "H_compressed_zstd = zarr.array(H, chunks=(679, H.shape[1]), \n",
    "                               compressor=zarr.Blosc(cname='zstd', clevel=1, shuffle=1))\n",
    "H_compressed_zstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as they are, these haplotype data can be compressed down to 6.1M.\n",
    "\n",
    "Now let's built the *PBWT*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pbwt, ppa, div = build_pbwt(np.asarray(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pbwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pbwt_compressed_zstd = zarr.array(pbwt, chunks=(679, H.shape[1]), \n",
    "                                  compressor=zarr.Blosc(cname='zstd', clevel=1, shuffle=2))\n",
    "pbwt_compressed_zstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this case, *PBWT* is more compressible than the original data, but only marginally, requiring 4.5M rather than 6.1M. This seems at odds with what is reported in Durbin (2014), i.e., that PBWT is many times more compressible than raw haplotypes, so what's going on? \n",
    "\n",
    "I think that a lot of the benefit in terms of compressibility that is reported in Durbin (2014) is actually due to the fact that the haplotype data in the PBWT are transposed relative to the original .gz representation. I.e., If haplotypes are organised one variant per row rather than one haplotype per row, and the underlying bytes are in row-major order, then the data become more compressible as you add more haplotypes, because variants tend to be rare and hence most rows will be composed almost entirely of zeros. If you then permute each row via the PBWT you get a further benefit, because PBWT tends to bring the ones and zeros together, however the added benefit is fairly marginal and most of the gains come simply from the transposed layout.\n",
    "\n",
    "Note that I am using Zstandard here and not the run length encoding used in Durbin (2014), I don't know how much difference that would make.\n",
    "\n",
    "To make use of the PBWT you also need the divergence array, so how compressible is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "div_compressed_zstd = zarr.array(div, chunks=(679, H.shape[1]), \n",
    "                                 compressor=zarr.Blosc(cname='zstd', clevel=1, shuffle=1))\n",
    "div_compressed_zstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The divergence array is not so compressible with this compression configuration. However Durbin (2014) suggests using a delta filter. Let's try this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "div_compressed_zstd_delta = zarr.array(div, chunks=(679, H.shape[1]), \n",
    "                                       compressor=zarr.Blosc(cname='zstd', clevel=1, shuffle=1),\n",
    "                                       filters=[zarr.Delta(dtype=div.dtype)])\n",
    "div_compressed_zstd_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, including a delta filter actually makes things worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw .ipynb for this post is [here](https://github.com/alimanfoo/alimanfoo.github.io/tree/master/_posts).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now().isoformat())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
